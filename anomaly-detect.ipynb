{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Malware in TLS with Machine Learning\r\n",
    "- Presented by Bryan Scarbrough\r\n",
    "- GSEC, GCIH, GCIA, GPYC, GXPN, GNFA, GCCC\r\n",
    "- Masterâ€™s Degree Candidate at the SANS Technology Institute\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "### Import Modules and Data\r\n",
    "\r\n",
    "- First stage of analysis is to import the required modules\r\n",
    "- Create a function to import the data for processing\r\n",
    "- Then print the dataset to verify successful import\r\n",
    "\r\n",
    "---\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\r\n",
    "\r\n",
    "# Import some initial libraries to get started\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import numpy as np\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "\r\n",
    "from pathlib import Path\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "\r\n",
    "# Set a random state variable for selection consistency\r\n",
    "rand_state = 42\r\n",
    "\r\n",
    "# Set malware label\r\n",
    "label = 'malware_label'\r\n",
    "\r\n",
    "# Function to import and format the dataset for processing\r\n",
    "def get_data(sample_size, mal_percent=20, scaled=False):\r\n",
    "    csv_data_file = r'./data/test_train_data.csv'\r\n",
    "    full_dataset = pd.read_csv(csv_data_file)\r\n",
    "\r\n",
    "    # Scale data to 0-1 value for more efficient ML analysis\r\n",
    "    if scaled:\r\n",
    "        mm_data = MinMaxScaler().fit_transform(full_dataset)\r\n",
    "        full_dataset = pd.DataFrame(mm_data, columns=full_dataset.columns)\r\n",
    "\r\n",
    "    # Convert label values to 1 and -1 (this is how OC-SVM performs predictions\r\n",
    "    # so validation requires these values\r\n",
    "    ben = 1\r\n",
    "    mal = -1\r\n",
    "    full_dataset.loc[full_dataset[label] == 1, label] = mal\r\n",
    "    full_dataset.loc[full_dataset[label] == 0, label] = ben\r\n",
    "\r\n",
    "    # Split dataset into benign and malware\r\n",
    "    benign = full_dataset[full_dataset.malware_label == ben]\r\n",
    "    malware = full_dataset[full_dataset.malware_label == mal]\r\n",
    "    \r\n",
    "    # Determine malware percentage of sample dataset size\r\n",
    "    mal_size = int((mal_percent / 100) * sample_size)\r\n",
    "\r\n",
    "    # Prevent malware sample size from being larger than actual sample size\r\n",
    "    if mal_size > malware.shape[0]:\r\n",
    "        mal_size = malware.shape[0]\r\n",
    "\r\n",
    "    # Generate random sample of malware of size determined by mal_size variable\r\n",
    "    malware = malware.sample(n=mal_size, random_state=rand_state)\r\n",
    "\r\n",
    "    # Prevent total sample size from being larger than actual sample size\r\n",
    "    total_sample_size = sample_size - mal_size\r\n",
    "    if total_sample_size > benign.shape[0]:\r\n",
    "        total_sample_size = benign.shape[0]\r\n",
    "\r\n",
    "    # Now generate a benign data sample and combine the benign and malware samples\r\n",
    "    # to a single returned dataset\r\n",
    "    benign = benign.sample(n=total_sample_size, random_state=rand_state)\r\n",
    "    sampled_data = benign.append(malware).reset_index(drop=True)\r\n",
    "    sampled_data = sampled_data.sample(frac=1).reset_index(drop=True)\r\n",
    "\r\n",
    "    # Return sample dataset\r\n",
    "    return sampled_data\r\n",
    "\r\n",
    "# Generate initial full dataset for analysis\r\n",
    "full_dataset = get_data(117000)\r\n",
    "\r\n",
    "# Drop rows with NaN (improperly calculated values rendered as Not-a-Number)\r\n",
    "full_dataset = full_dataset.dropna()\r\n",
    "\r\n",
    "# Print the dataset\r\n",
    "print(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "## Data Analysis\r\n",
    "\r\n",
    "Before conducting any ML processing it is:\r\n",
    "\r\n",
    "  - Necessary to understand the nature of the data's relationships\r\n",
    "  - Determine the significance of various data features and how they are related to one another\r\n",
    "\r\n",
    "### Data Distribution\r\n",
    "\r\n",
    "Below is the percent distribution of Malware to Benign data.\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column to analyze\n",
    "target = full_dataset[label]\n",
    "# Get dataset length for percentage calculation\n",
    "total = len(full_dataset)\n",
    "# Define graph area and title\n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.title(\"Malware Dataset Distribution\")\n",
    "\n",
    "# Generate count plot and turn into bar graph for display\n",
    "ax = sns.countplot(target)\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.0f}%'.format(p.get_height() / total * 100)\n",
    "    x = p.get_x() + p.get_width() / 2\n",
    "    y = p.get_height() + 5\n",
    "    ax.annotate(percentage, (x, y), ha = 'center')\n",
    "    ax.set_xticklabels(['Benign', 'Malware'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "## Cipher Suite Analysis\r\n",
    "\r\n",
    "- Time to analyze some of the individual features\r\n",
    "- Begin with Cipher Suties (the metadata value with the largest number of features)\r\n",
    "- The first graph shows the number of unique cipher suites used by each data classification (benign and malware)\r\n",
    "- The bottom graphs represent the most used cipher suites sorted by classification\r\n",
    "  - Left: Benign Cipher Suties used\r\n",
    "  - Right: Malicious Cipher Suites used\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cipher_suites():\r\n",
    "    # Separate dataset by malware label\r\n",
    "    mal = full_dataset[full_dataset.malware_label == -1]\r\n",
    "    ben = full_dataset[full_dataset.malware_label == 1]\r\n",
    "\r\n",
    "    # Select only Cipher Suite columns\r\n",
    "    mal_cs_data = mal.filter(regex='cs_')\r\n",
    "    ben_cs_data = ben.filter(regex='cs_')\r\n",
    "\r\n",
    "    # Average Cipher Suite length value to graph in next cell\r\n",
    "    mal_cs_size = round(mal_cs_data['cs_len'].sum() / len(mal_cs_data['cs_len']), 2)\r\n",
    "    ben_cs_size = round(ben_cs_data['cs_len'].sum() / len(ben_cs_data['cs_len']), 2)\r\n",
    "\r\n",
    "    # Drop Cipher Suite length field - not related to number of CS values offered by client\r\n",
    "    mal_cs_data = mal_cs_data.drop(['cs_len'], axis=1)\r\n",
    "    ben_cs_data = ben_cs_data.drop(['cs_len'], axis=1)\r\n",
    "\r\n",
    "    # Drop columns containing only \"0\" values - these are CS values never offered by clients\r\n",
    "    # Then determine the length of remaining values to determine number of CS values\r\n",
    "    mal_cs_count = len(mal_cs_data.loc[:, (mal_cs_data != 0).any(axis=0)].columns)\r\n",
    "    ben_cs_count = len(ben_cs_data.loc[:, (ben_cs_data != 0).any(axis=0)].columns)\r\n",
    "\r\n",
    "    # Create graph container\r\n",
    "    fig, axes = plt.subplots(2, 2, sharex=False, figsize=(15,12))\r\n",
    "\r\n",
    "    # Calculate total number unique of Benign and Malware Cipher Suites used\r\n",
    "    cs_count = pd.DataFrame({'Unique Cipher Suites': ['Benign Cipher Suites', 'Malware Cipher Suites'], 'Count': [ben_cs_count, mal_cs_count]})\r\n",
    "    ax = sns.barplot(ax=axes[0,0], data=cs_count, y='Count', x='Unique Cipher Suites')\r\n",
    "    for bar in ax.patches:\r\n",
    "        ax.annotate(format(bar.get_height(), ''),\r\n",
    "                        (bar.get_x() + bar.get_width() / 2,  \r\n",
    "                        bar.get_height()), ha='center', va='center', \r\n",
    "                        size=15, xytext=(0, 8), \r\n",
    "                        textcoords='offset points')\r\n",
    "\r\n",
    "    # Sort cipher suites used by Benign importance\r\n",
    "    ben_cs_sum = pd.DataFrame({\"Benign\": (ben_cs_data.sum() / len(ben_cs_data)), \"Malware\": (mal_cs_data.sum() / len(mal_cs_data))}, index=ben_cs_data.sum().sort_values().index).tail(20)\r\n",
    "    ben_cs_sum.plot.barh(ax=axes[1,0], rot=0)\r\n",
    "    axes[1,0].set_xlabel('Cipher Suites by Benign Importance')\r\n",
    "\r\n",
    "    # Sorce cipher suites used by Malware importance\r\n",
    "    mal_cs_sum = pd.DataFrame({\"Benign\": (ben_cs_data.sum() / len(ben_cs_data)), \"Malware\": (mal_cs_data.sum() / len(mal_cs_data))}, index=mal_cs_data.sum().sort_values().index).tail(20)\r\n",
    "    mal_cs_sum.plot.barh(ax=axes[1,1], rot=0)\r\n",
    "    axes[1,1].set_xlabel('Cipher Suites by Malware Importance')\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Cipher Suite relationships\r\n",
    "cipher_suites()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "## Port Analysis\r\n",
    "\r\n",
    "- The next section of features worthy of analysis are the ports used\r\n",
    "\r\n",
    "> NOTE: While these values are easily changeable by an attacker, they can easily be overlooked, and for the purposes of this research offer a significant statistical variation leading to successful ML analysis.\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ports():\r\n",
    "    # PORT ANALYSIS\r\n",
    "    #\r\n",
    "    # Create dataframe of malicious and benign source and destination ports\r\n",
    "    mal_prt = mal[['src_port', 'dst_port']]\r\n",
    "    ben_prt = ben[['src_port', 'dst_port']]\r\n",
    "\r\n",
    "    # Create graph container\r\n",
    "    fig, axes = plt.subplots(3, 2, sharex=False, figsize=(15,15))\r\n",
    "\r\n",
    "    # Calculate and graph number of unique destination ports\r\n",
    "    ben_dst_percent = ben_prt.dst_port.unique().shape[0]\r\n",
    "    mal_dst_percent = mal_prt.dst_port.unique().shape[0]\r\n",
    "    dst_prt_sum = pd.DataFrame({'Unique Destination Ports': ['Benign Unique Dst Ports', 'Malware Unique Dst Ports'], 'Count': [ ben_dst_percent, mal_dst_percent]})\r\n",
    "    ax = sns.barplot(ax=axes[0,0], data=dst_prt_sum, y='Count', x='Unique Destination Ports')\r\n",
    "    for bar in ax.patches:\r\n",
    "        ax.annotate(format(bar.get_height(), ''),\r\n",
    "                        (bar.get_x() + bar.get_width() / 2,  \r\n",
    "                        bar.get_height()), ha='center', va='center', \r\n",
    "                        size=15, xytext=(0, 8), \r\n",
    "                        textcoords='offset points')\r\n",
    "\r\n",
    "    # Calculate and graph number of unique source ports\r\n",
    "    ben_src_percent = (ben_prt.src_port.unique().shape[0] / len(ben_prt.src_port)) * 100\r\n",
    "    mal_src_percent = (mal_prt.src_port.unique().shape[0] / len(mal_prt.src_port)) * 100\r\n",
    "    src_prt_sum = pd.DataFrame({'Unique Source Ports': ['Benign Unique Src Ports', 'Malware Unique Src Ports'], 'Count': [ ben_src_percent, mal_src_percent]})\r\n",
    "    ax = sns.barplot(ax=axes[0,1], data=src_prt_sum, y='Count', x='Unique Source Ports')\r\n",
    "    for bar in ax.patches:\r\n",
    "        ax.annotate('{:.2f}%'.format(bar.get_height(), ''),\r\n",
    "                        (bar.get_x() + bar.get_width() / 2,  \r\n",
    "                        bar.get_height()), ha='center', va='center', \r\n",
    "                        size=15, xytext=(0, 8), \r\n",
    "                        textcoords='offset points')\r\n",
    "\r\n",
    "    # Determine top benign destination ports\r\n",
    "    ben_dst_prt = pd.DataFrame(ben_prt.dst_port.astype(int).value_counts().sort_values())\r\n",
    "    ax = sns.barplot(ax=axes[1,0], data=cs_count, y=ben_dst_prt.dst_port, x=ben_dst_prt.index, orient='v')\r\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\r\n",
    "    ax.set(xlabel='Benign Destionation Ports', ylabel='Count')\r\n",
    "\r\n",
    "    # Determine top malware destination ports\r\n",
    "    mal_dst_prt = pd.DataFrame(mal_prt.dst_port.astype(int).value_counts().sort_values()).tail(10)\r\n",
    "    ax = sns.barplot(ax=axes[1,1], data=cs_count, y=mal_dst_prt.dst_port, x=mal_dst_prt.index, orient='v')\r\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\r\n",
    "    ax.set(xlabel='Malware Destination Ports', ylabel='Count')\r\n",
    "\r\n",
    "    # Determine top benign source ports\r\n",
    "    ben_src_prt = pd.DataFrame(ben_prt.src_port.astype(int).value_counts()).head(10)\r\n",
    "    ax = sns.barplot(ax=axes[2,0], data=cs_count, y=ben_src_prt.src_port, x=ben_src_prt.index, orient='v')\r\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\r\n",
    "    ax.set(xlabel='Benign Source Ports', ylabel='Count')\r\n",
    "\r\n",
    "    # Determine top malware sourceports\r\n",
    "    mal_src_prt = pd.DataFrame(mal_prt.src_port.astype(int).value_counts()).head(10)\r\n",
    "    ax = sns.barplot(ax=axes[2,1], data=cs_count, y=mal_src_prt.src_port, x=mal_src_prt.index, orient='v')\r\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\r\n",
    "    ax.set(xlabel='Malware Source Ports', ylabel='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph port relationships\r\n",
    "ports()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "### How a Support Vector Machine (SVM) Works...\r\n",
    "\r\n",
    "- Binary classification system\r\n",
    "- Calculates best line (or plane, also called hyperplane) to separate data\r\n",
    "- Measures distance between nearest samples to hyperplane\r\n",
    "  - Measurement determines the margin\r\n",
    "  - Nearest samples called the Support Vectors\r\n",
    "- When data is non-linear (cannot separate with straight line), SVM uses \"kernels\" to classify\r\n",
    "\r\n",
    "<img src=\"./images/svm-margin.jpg\" alt=\"Margin\" style=\"width: 400px;\" />\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "test_percent = 20\r\n",
    "\r\n",
    "def graph_svm():\r\n",
    "    # Import truncated dataset for expedited demonstration\r\n",
    "    formatted_data = get_data(50000, 125, True)\r\n",
    "\r\n",
    "    # Separate labels from feature data\r\n",
    "    label_data = formatted_data.malware_label\r\n",
    "    feature_data = formatted_data.drop(label, axis=1)\r\n",
    "\r\n",
    "    # Perform PCA reduction to 2 components\r\n",
    "    pca = PCA(n_components=2).fit_transform(feature_data)\r\n",
    "    data = pd.DataFrame(pca)\r\n",
    "\r\n",
    "    # Split data into test and training subsets\r\n",
    "    svm_x_train, svm_x_test, svm_y_train, svm_x_test = train_test_split(data, label_data, test_size=(test_percent / 100), random_state=rand_state)\r\n",
    "\r\n",
    "    # Train/Fit the SVM model\r\n",
    "    svclassifier = SVC(kernel='rbf', C=1, gamma=0.1, probability=True, random_state=42)\r\n",
    "    svclassifier.fit(svm_x_train, svm_y_train)\r\n",
    "    x_data = np.array(data)\r\n",
    "\r\n",
    "    # Generate the Margin graph\r\n",
    "    plt.figure(figsize=(12,12))\r\n",
    "    plt.scatter(x_data[:, 0], x_data[:, 1], c=label_data.values, s=40, cmap=plt.cm.winter, alpha=0.5)\r\n",
    "    ax = plt.gca()\r\n",
    "    xlim = [-2, 2]\r\n",
    "    ylim = [-5, 4]\r\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\r\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\r\n",
    "    XX, YY = np.meshgrid(xx, yy)\r\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\r\n",
    "    Z = svclassifier.decision_function(xy).reshape(XX.shape)\r\n",
    "\r\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\r\n",
    "    ax.scatter(svclassifier.support_vectors_[:, 0], svclassifier.support_vectors_[:, 1],\r\n",
    "                s=100, linewidth=1, facecolors='none', edgecolors='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the SVM Margin graph\r\n",
    "graph_svm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "### The One-Class SVM\r\n",
    "\r\n",
    "- Imbalanced datasets are an intrinsic problem with many ML algorithms such as the SVM\r\n",
    "  - A significant imbalance between data classes makes ML model struggle to function\r\n",
    "  - The SVM has problems calculating the margin\r\n",
    "- One-Class SVM, or OC-SVM, solves this problem\r\n",
    "  - Uses only the majority class to designate the decision boundary\r\n",
    "  - Treats anything outside the boundary as an anomaly\r\n",
    "\r\n",
    "<img src=\"./images/oc-svm-margin.png\" alt=\"Margin\" style=\"width: 420px;display: inline-block;\" /><img src=\"./images/oc-svm-classification.png\" alt=\"Classification\" style=\"width: 420px;display: inline-block;\" />\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Generate new random dataset\n",
    "formatted_data = get_data(50000, 0.2, True)\n",
    "\n",
    "# Split dataset by label - the OC-SVM is trained with Benign data ONLY\n",
    "oc_benign = formatted_data[formatted_data.malware_label == 1]\n",
    "oc_malware = formatted_data[formatted_data.malware_label == -1]\n",
    "\n",
    "# Split dataset into test and training \n",
    "oc_b_train, oc_b_test = train_test_split(oc_benign, test_size=(test_percent / 100), random_state=rand_state)\n",
    "oc_b_train = oc_b_train.drop(label, axis=1)\n",
    "\n",
    "# Set nu and gamma hyperparameters (nu is expected percentage of malware in dataset)\n",
    "nu_value = (len(oc_malware) / len(oc_b_test))\n",
    "gamma_val = 0.1\n",
    "\n",
    "# Combine malware with test sample for analysis\n",
    "oc_test = oc_b_test.append(oc_malware)\n",
    "oc_test_label = oc_test.malware_label\n",
    "oc_test = oc_test.drop(label, axis=1)\n",
    "\n",
    "# Train/Fit OC-SVM data model\n",
    "svclassifier = OneClassSVM(nu=nu_value, kernel='rbf', gamma=gamma_val)\n",
    "svclassifier.fit(oc_b_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "### OC-SVM Scores\r\n",
    "\r\n",
    "- Calculate how the OC-SVM performs (not speed, but correctness)\r\n",
    "- Scores used:\r\n",
    "  - Accuracy\r\n",
    "  - Precision\r\n",
    "  - Recall\r\n",
    "  - F2 Score\r\n",
    "\r\n",
    "<img src=\"./images/precision-recall-relevance.png\" alt=\"Precision-Recall\" style=\"width: 800px;\" />\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Calculate F-scores\n",
    "def f_beta(beta, precision, recall):\n",
    "    return (beta*beta + 1) * precision * recall / (beta * beta * precision + recall)\n",
    "\n",
    "# Perform n-fold cross validation and calculate the mean score across the cv=## folds\n",
    "print('\\nCalculating OC-SVM scores...')\n",
    "for val in ['accuracy', 'precision', 'recall']:\n",
    "    score = cross_val_score(svclassifier, oc_test, oc_test_label, cv=5, scoring=val).mean()\n",
    "    print(\"{}: {}\".format(val, score))\n",
    "    if val == 'precision':\n",
    "        prec = score\n",
    "    elif val == 'recall':\n",
    "        rec = score\n",
    "print(\"F2 Score: {}\".format(f_beta(2.0, prec, rec)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "### OC-SVM Performance Graphs\r\n",
    "\r\n",
    "- Finally, visualize OC-SVM performance\r\n",
    "- Left graph is Confusion Matrix showing\r\n",
    "  - True-Positive\r\n",
    "  - True-Negative\r\n",
    "  - False-Positive\r\n",
    "  - False-Negative\r\n",
    "- Right graph is Reciver Operating Curve (ROC) graph\r\n",
    "  - Represents F1-score of model\r\n",
    "  - Everything \"inside\" or \"below\" the yellow line is correctly classified\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\r\n",
    "\r\n",
    "def graph_oc_svm():\r\n",
    "    oc_pred = svclassifier.predict(oc_test)\r\n",
    "    ben = 0\r\n",
    "    mal = 1\r\n",
    "    oc_pred[oc_pred == 1] = 0\r\n",
    "    oc_pred[oc_pred == -1] = 1\r\n",
    "\r\n",
    "    oc_test_label[oc_test_label == 1] = 0\r\n",
    "    oc_test_label[oc_test_label == -1] = 1\r\n",
    "\r\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharex=False, figsize=(15,8))\r\n",
    "\r\n",
    "    # Generate AUC graph\r\n",
    "    fpr, tpr, _ = roc_curve(oc_test_label, oc_pred)\r\n",
    "    auc = roc_auc_score(oc_test_label, oc_pred)\r\n",
    "    plt.plot([0,1], [0,1], color='navy', lw=2, linestyle='--')\r\n",
    "    plt.xlim([-0.05, 1.0])\r\n",
    "    plt.ylim([0.0, 1.05])\r\n",
    "    plt.xlabel('False Positive Rate')\r\n",
    "    plt.ylabel('True Positive Rate')\r\n",
    "    plt.plot(fpr, tpr, label='ROC Curve (area = {})'.format(str(auc)), color='darkorange')\r\n",
    "    plt.legend(loc='lower right')\r\n",
    "    plt.title('Receiver Operating Characteristic (ROC curve)')\r\n",
    "\r\n",
    "    # Generate Confusion Matrix\r\n",
    "    conf_matrix = confusion_matrix(oc_test_label, oc_pred)\r\n",
    "    sns.heatmap(conf_matrix, ax=ax1,\r\n",
    "            xticklabels=['Benign', 'Malware'],\r\n",
    "            yticklabels=['Benign', 'Malware'],\r\n",
    "            annot=True, fmt='d')\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Confusion Matrix and ROC graph\r\n",
    "graph_oc_svm()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}